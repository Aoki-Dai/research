# Wikipedia学習済みWord2Vecを用いたマッチング結果の評価と考察

## 1. 概要
本レポートでは、`notebooks/matching_wiki.ipynb` で実施された、Wikipediaで学習したWord2Vecモデルを用いたプロジェクトとユーザーのマッチング結果について評価・考察を行います。

## 2. 評価結果

### 2.1. マッチングの妥当性（定性評価）
いくつかの事例を確認すると、キーワードレベルでのマッチングはある程度機能していることが確認できます。

*   **ケース1: 旅行予約システム設計 (QAエンジニア)**
    *   **1位 User 259**: 過去に「スマホアプリ開発」で「QAエンジニア」の経験あり。
    *   **評価**: 職種名が直接一致しており、妥当なマッチングと言えます。

*   **ケース2: 予約管理システム設計 (フルスタックエンジニア)**
    *   **1位 User 259**: 再びUser 259が選出。
    *   **3位 User 140**: 「ECサイトリニューアル」で「バックエンドAPIの設計と実装(Node.js/Django)」の経験あり。
    *   **評価**: User 259はマネジメントやQAの経験が主ですが、User 140のような実装経験豊富なユーザーも上位に含まれており、ある程度関連性は捉えられています。

*   **ケース3: ブロックチェーン開発実装 (フルスタックエンジニア)**
    *   **1位 User 88**: 「セキュリティ強化プロジェクト」の経験あり。プロジェクト説明の「セキュリティ強化」というキーワードに反応した可能性があります。
    *   **評価**: 必ずしもブロックチェーンの専門家ではないかもしれませんが、周辺スキル（セキュリティ等）でのマッチングが見られます。

### 2.2. スコアの分布と識別性
*   **平均類似度**: 0.9212
*   **標準偏差**: 0.0217
*   **最小値**: 0.7774
*   **考察**: 全体的に類似度が **極めて高い（0.9以上）** 傾向にあります。これは、Wikipediaという汎用的なコーパスで学習されたモデルでは、IT関連の単語がベクトル空間上の狭い領域に密集してしまっている可能性を示唆しています。その結果、プロジェクトとユーザーの差異を十分に識別できていない（Discriminativeでない）恐れがあります。

### 2.3. 特定ユーザーへの偏り（Super User問題）
マッチング回数の上位ユーザーを確認すると、特定のユーザーに推薦が集中していることがわかります。

*   **User 75**: 197回推薦（全プロジェクトの約20%）
*   **User 88**: 193回推薦
*   **User 230**: 138回推薦

**考察**:
特定の「汎用的な用語」や「人気のあるキーワード」を多く含む経歴を持つユーザーが、あらゆるプロジェクトに対して高い類似度を持ってしまっている可能性があります。これにより、一部のユーザーにオファーが集中し、他のユーザーが推薦されないという不均衡が生じています。

## 3. 考察と今後の改善案

### 3.1. 課題
1.  **ドメイン適応の不足**: Wikipediaモデルは一般的すぎるため、ITエンジニアリングの文脈（言語、フレームワーク、役割の微妙な違い）を正確に捉えきれていない可能性があります。
2.  **ハブ化現象 (Hubness)**: 高次元ベクトル空間において、特定のベクトルが多くのベクトルと高い類似度を持つ現象が発生していると考えられます。
3.  **語彙の重複**: 単純な単語の平均ベクトルを使用しているため、文章量が多いユーザーや、一般的なIT用語を多用するユーザーが有利になっている可能性があります。

### 3.2. 改善案
1.  **ドメイン特化モデルの利用**:
    *   Wikipediaではなく、技術記事（Qiita, Zenn, StackOverflowなど）や実際の職務経歴書データを用いてWord2Vecを学習し直す、あるいはファインチューニングする。
2.  **IDFによる重み付け**:
    *   `create_document_vector` 関数において、単語ベクトルの平均を取る際に、TF-IDFのIDF値などで重み付けを行う。これにより、「開発」「システム」といった一般的な単語の影響を下げ、「ブロックチェーン」「Rust」といった専門性の高い単語の影響を強めることができます。
3.  **より高度なモデルの利用**:
    *   単語の集合（BoW）的なアプローチであるWord2Vecの平均ではなく、文脈を考慮できる **SBERT (Sentence-BERT)** などのTransformerベースのモデルを利用する。
4.  **多様性の確保**:
    *   マッチングロジックに、特定のユーザーばかりが選ばれないようなペナルティ項や、MMR (Maximal Marginal Relevance) のような多様性を考慮したリランキングを導入する。

## 4. 結論
現在のWikipedia学習済みWord2Vecモデルによるマッチングは、基本的なキーワードマッチングとしては機能していますが、精度と公平性の観点からは改善の余地が大きいです。特に「全ユーザーが高類似度になる」「特定ユーザーへの集中」という課題は、実運用においては致命的になり得るため、ドメイン特化や重み付けの工夫が必要です。
